{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88107,"databundleVersionId":10074745,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AOML Assignment 2\n# Team Reshiram_119_909_912_920\n\n### Prarthana Prakash Kini : PES1UG22AM119\n### Tejas V Bhat : PES1UG22AM909\n### Ayush Muralidharan : PES1UG22AM912\n### Atharv Revankar : PES1UG22AM920","metadata":{}},{"cell_type":"code","source":"pip install umap-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:26.132031Z","iopub.execute_input":"2024-11-12T17:47:26.132475Z","iopub.status.idle":"2024-11-12T17:47:40.621913Z","shell.execute_reply.started":"2024-11-12T17:47:26.132433Z","shell.execute_reply":"2024-11-12T17:47:40.620188Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Importing necessary python modules and methods","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import SpectralClustering\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.metrics.pairwise import rbf_kernel\nimport umap\nfrom sklearn.mixture import BayesianGaussianMixture\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.625215Z","iopub.execute_input":"2024-11-12T17:47:40.625839Z","iopub.status.idle":"2024-11-12T17:47:40.636313Z","shell.execute_reply.started":"2024-11-12T17:47:40.625769Z","shell.execute_reply":"2024-11-12T17:47:40.634856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading and analysis","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/aoml-assignment-2-clustering/data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.638144Z","iopub.execute_input":"2024-11-12T17:47:40.638740Z","iopub.status.idle":"2024-11-12T17:47:40.661990Z","shell.execute_reply.started":"2024-11-12T17:47:40.638691Z","shell.execute_reply":"2024-11-12T17:47:40.660357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.665078Z","iopub.execute_input":"2024-11-12T17:47:40.665496Z","iopub.status.idle":"2024-11-12T17:47:40.683705Z","shell.execute_reply.started":"2024-11-12T17:47:40.665452Z","shell.execute_reply":"2024-11-12T17:47:40.682287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.685443Z","iopub.execute_input":"2024-11-12T17:47:40.685949Z","iopub.status.idle":"2024-11-12T17:47:40.724319Z","shell.execute_reply.started":"2024-11-12T17:47:40.685851Z","shell.execute_reply":"2024-11-12T17:47:40.722993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.725853Z","iopub.execute_input":"2024-11-12T17:47:40.726226Z","iopub.status.idle":"2024-11-12T17:47:40.736304Z","shell.execute_reply.started":"2024-11-12T17:47:40.726187Z","shell.execute_reply":"2024-11-12T17:47:40.734810Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### StandardScaler is applied to standardize the features, ensuring they have a mean of 0 and a standard deviation of 1\n\n","metadata":{}},{"cell_type":"code","source":"unique_ids = data['id']\nfeatures = data\n\n\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.737748Z","iopub.execute_input":"2024-11-12T17:47:40.738162Z","iopub.status.idle":"2024-11-12T17:47:40.754308Z","shell.execute_reply.started":"2024-11-12T17:47:40.738122Z","shell.execute_reply":"2024-11-12T17:47:40.753028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.755882Z","iopub.execute_input":"2024-11-12T17:47:40.756468Z","iopub.status.idle":"2024-11-12T17:47:40.780004Z","shell.execute_reply.started":"2024-11-12T17:47:40.756414Z","shell.execute_reply":"2024-11-12T17:47:40.778165Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### As we have 5 features we must perform dimensionality reduction for effective clustering and visualisation.\n#### We have experimented with 3 dimensionality reduction techniques, both linear and non linear:","metadata":{}},{"cell_type":"markdown","source":"### 1. Dimensionality Reduction with PCA\nA PCA model is created to reduce the feature set to 2 principal components, which capture the maximum variance in a 2-dimensional space.\n- n_components=2: Specifies 2 principal components.\n- tol=0.01: Sets the tolerance level for convergence.\n- random_state=5: Ensures reproducibility of results","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=2, \n          tol=0.01, \n          random_state=5\n)\npca_features = pca.fit_transform(scaled_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.782026Z","iopub.execute_input":"2024-11-12T17:47:40.782671Z","iopub.status.idle":"2024-11-12T17:47:40.799175Z","shell.execute_reply.started":"2024-11-12T17:47:40.782604Z","shell.execute_reply":"2024-11-12T17:47:40.797366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Dimensionality Reduction with t-SNE\n\n- TSNE is configured with n_components=2 to reduce the data to 2 dimensions for visualization.\n- perplexity=100 controls the balance between local and global data structure.","metadata":{}},{"cell_type":"code","source":"tsne = TSNE(n_components=2,perplexity = 300)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.805336Z","iopub.execute_input":"2024-11-12T17:47:40.806569Z","iopub.status.idle":"2024-11-12T17:47:40.813138Z","shell.execute_reply.started":"2024-11-12T17:47:40.806503Z","shell.execute_reply":"2024-11-12T17:47:40.811183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tsne_features = tsne.fit_transform(scaled_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:47:40.815043Z","iopub.execute_input":"2024-11-12T17:47:40.815830Z","iopub.status.idle":"2024-11-12T17:48:10.358169Z","shell.execute_reply.started":"2024-11-12T17:47:40.815745Z","shell.execute_reply":"2024-11-12T17:48:10.357052Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Dimensionality Reduction with UMAP\n\n- n_components=2: Reduces the data to 2 dimensions for visualization and clustering.\n- min_dist=0.01: Controls how closely points are packed together. A smaller value allows points to be closer, creating tighter clusters.\n- n_epochs=500: Sets the number of training epochs, where a higher value may improve embedding stability.\n- random_state=42: Ensures reproducibility of the results by using a fixed seed.","metadata":{}},{"cell_type":"code","source":"umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1,learning_rate=0.01, random_state=42)\numap_features = umap_reducer.fit_transform(scaled_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:10.359666Z","iopub.execute_input":"2024-11-12T17:48:10.360822Z","iopub.status.idle":"2024-11-12T17:48:18.059222Z","shell.execute_reply.started":"2024-11-12T17:48:10.360742Z","shell.execute_reply":"2024-11-12T17:48:18.057446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### We have chosen TSNE as we have found it to give the most optimum performace for clustering.","metadata":{}},{"cell_type":"markdown","source":"## Spectral Clustering with RBF Kernel\n\nSpectral clustering is a technique that uses the eigenvalues of a similarity matrix (in this case, the RBF kernel) to perform dimensionality reduction before applying a clustering algorithm, such as KMeans. It is particularly useful for non-linearly separable clusters.\n\n Compute RBF Kernel:\n   - gamma_value: The gamma parameter controls the influence of each point in the RBF kernel. \n   - rbf_kernel: Computes the RBF (Radial Basis Function) kernel matrix on the UMAP-reduced features (umap_features), capturing pairwise similarities.\n\n Apply Spectral Clustering:\n   - n_clusters: Specifies the desired number of clusters.\n   - affinity: Uses the precomputed RBF kernel matrix as the similarity measure for clustering.\n   - assign_labels: Labels are assigned to clusters using the KMeans algorithm.\n   - random_state: Ensures reproducibility by setting a fixed seed.\n\n","metadata":{}},{"cell_type":"code","source":"gamma_value = 1/2 \naffinity_matrix = rbf_kernel(tsne_features, gamma=gamma_value)\n\nspectral_clustering = SpectralClustering(\n    n_clusters=6,  \n    affinity='precomputed',  \n    assign_labels='kmeans', \n    random_state=42\n)\nspectral_labels = spectral_clustering.fit_predict(affinity_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T17:48:18.060713Z","iopub.execute_input":"2024-11-12T17:48:18.061151Z","iopub.status.idle":"2024-11-12T17:48:19.796494Z","shell.execute_reply.started":"2024-11-12T17:48:18.061109Z","shell.execute_reply":"2024-11-12T17:48:19.795461Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate Clustering Quality:\n   - Silhouette Score: Measures how similar each point is to its own cluster compared to other clusters. Higher values indicate better clustering.\n   - Davies-Bouldin Score: A lower value indicates better clustering. This metric evaluates both intra-cluster similarity and inter-cluster separation.\n\n","metadata":{}},{"cell_type":"code","source":"silhouette_spectral = silhouette_score(tsne_features, spectral_labels)\ndavies_bouldin_spectral = davies_bouldin_score(tsne_features, spectral_labels)\nprint(f\"Spectral Silhouette Score: {silhouette_spectral}\")\nprint(f\"Spectral Davies-Bouldin Score: {davies_bouldin_spectral}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:19.801970Z","iopub.execute_input":"2024-11-12T17:48:19.804495Z","iopub.status.idle":"2024-11-12T17:48:19.939436Z","shell.execute_reply.started":"2024-11-12T17:48:19.804440Z","shell.execute_reply":"2024-11-12T17:48:19.937868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(x=tsne_features[:, 0], y=tsne_features[:, 1], hue=spectral_labels, palette='Set2')\nplt.title(\"TSNE Spectral Cluster Visualization\")\nplt.xlabel(\"TSNE Component 1\")\nplt.ylabel(\"TSNE Component 2\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:19.947483Z","iopub.execute_input":"2024-11-12T17:48:19.952367Z","iopub.status.idle":"2024-11-12T17:48:20.538768Z","shell.execute_reply.started":"2024-11-12T17:48:19.952263Z","shell.execute_reply":"2024-11-12T17:48:20.537517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## K-Means Clustering\n\nK-Means is a popular clustering algorithm that partitions the dataset into a specified number of clusters by continuously recalculating cluster centroids and grouping the points closest to each centroid into clusters.\n\n *Initialize K-Means*:\n   - *n_clusters*: Specifies the number of clusters to form, set to 6 as per the requirement.\n   - *init*: Uses the K-Means++ method for initialization, which improves convergence by choosing initial centroids that are distant from each other.\n   - *n_init*: The algorithm runs 10 times with different centroid seeds, and the best output in terms of inertia is selected.\n   - *max_iter*: Sets the maximum number of iterations for a single run, ensuring sufficient iterations for convergence.\n   - *random_state*: Ensures reproducibility by setting a fixed seed.\n\n *Apply K-Means*:\n   - *fit_predict*: Computes K-Means clustering on the UMAP-reduced features (umap_features) and assigns cluster labels (kmeans_labels) to each point.","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=6, \n                init='k-means++',\n                n_init=10, \n                max_iter=500, \n                random_state=42\n)\n\nkmeans_labels = kmeans.fit_predict(tsne_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:20.540515Z","iopub.execute_input":"2024-11-12T17:48:20.541011Z","iopub.status.idle":"2024-11-12T17:48:20.607303Z","shell.execute_reply.started":"2024-11-12T17:48:20.540967Z","shell.execute_reply":"2024-11-12T17:48:20.606090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"silhouette_kmeans = silhouette_score(tsne_features, kmeans_labels)\ndavies_bouldin_kmeans = davies_bouldin_score(tsne_features, kmeans_labels)\nprint(f\"K-Means Silhouette Score: {silhouette_kmeans}\")\nprint(f\"K-Means Davies-Bouldin Score: {davies_bouldin_kmeans}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:20.608990Z","iopub.execute_input":"2024-11-12T17:48:20.609398Z","iopub.status.idle":"2024-11-12T17:48:20.735286Z","shell.execute_reply.started":"2024-11-12T17:48:20.609355Z","shell.execute_reply":"2024-11-12T17:48:20.734017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(x=tsne_features[:, 0], y=tsne_features[:, 1], hue=kmeans_labels, palette='Set2')\nplt.title(\"TSNE Spectral Cluster Visualization\")\nplt.xlabel(\"TSNE Component 1\")\nplt.ylabel(\"TSNE Component 2\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:20.737033Z","iopub.execute_input":"2024-11-12T17:48:20.737864Z","iopub.status.idle":"2024-11-12T17:48:21.319806Z","shell.execute_reply.started":"2024-11-12T17:48:20.737784Z","shell.execute_reply":"2024-11-12T17:48:21.317559Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bayesian Gaussian Mixture (BGM) Clustering\n\nBayesian Gaussian Mixture (BGM) is a probabilistic clustering method that extends the Gaussian Mixture Model (GMM) by introducing Bayesian regularization. This regularization allows BGM to automatically determine the optimal number of clusters, making it robust for datasets with varying complexity.\n\nInitialize Bayesian Gaussian Mixture:\n   - n_components : Specifies the maximum number of clusters to be detected.\n   - covariance_type : Uses a full covariance matrix for each cluster, allowing it to model clusters of any shape and orientation.","metadata":{}},{"cell_type":"code","source":"bgm = BayesianGaussianMixture(\n    n_components=6,                     \n    covariance_type='full',               \n)\nbgm_labels = bgm.fit_predict(tsne_features)\n\nsilhouette_bgm = silhouette_score(tsne_features, bgm_labels)\ndavies_bouldin_bgm = davies_bouldin_score(tsne_features, bgm_labels)\nprint(f\"BayesianGaussianMixture Silhouette Score: {silhouette_bgm}\")\nprint(f\"K-Means Davies-Bouldin Score: {davies_bouldin_bgm}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:21.321459Z","iopub.execute_input":"2024-11-12T17:48:21.321890Z","iopub.status.idle":"2024-11-12T17:48:21.635538Z","shell.execute_reply.started":"2024-11-12T17:48:21.321844Z","shell.execute_reply":"2024-11-12T17:48:21.633841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(x=tsne_features[:, 0], y=tsne_features[:, 1], hue=bgm_labels, palette='Set2')\nplt.title(\"TSNE BGM Cluster Visualization\")\nplt.xlabel(\"TSNE Component 1\")\nplt.ylabel(\"TSNE Component 2\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:21.637277Z","iopub.execute_input":"2024-11-12T17:48:21.638530Z","iopub.status.idle":"2024-11-12T17:48:22.151499Z","shell.execute_reply.started":"2024-11-12T17:48:21.638468Z","shell.execute_reply":"2024-11-12T17:48:22.150067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gaussian Mixture Model (GMM) Clustering\n\nGaussian Mixture Model (GMM) is a probabilistic clustering algorithm that assumes data points are generated from a mixture of several Gaussian distributions. It models the data as a combination of multiple Gaussian components, making it effective for capturing complex cluster shapes.\n\n1. Initialize Gaussian Mixture Model:\n   - n_components=6: Specifies the number of Gaussian components (clusters) to fit in the data, set to 6.\n   - covariance_type='full': Uses a full covariance matrix for each component, allowing for clusters with arbitrary shapes.\n   - random_state=42: Ensures reproducibility by setting a fixed seed.","metadata":{}},{"cell_type":"code","source":"gmm = GaussianMixture(n_components=6, covariance_type='full', random_state=42)\ngmm.means_init = kmeans.cluster_centers_\ngmm.fit(tsne_features)\ngmm_labels = gmm.predict(tsne_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:22.153169Z","iopub.execute_input":"2024-11-12T17:48:22.153587Z","iopub.status.idle":"2024-11-12T17:48:22.201915Z","shell.execute_reply.started":"2024-11-12T17:48:22.153536Z","shell.execute_reply":"2024-11-12T17:48:22.200657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"silhouette_gmm = silhouette_score(tsne_features, gmm_labels)\ndavies_bouldin_gmm = davies_bouldin_score(tsne_features, gmm_labels)\nprint(f\"GMM Silhouette Score: {silhouette_gmm}\")\nprint(f\"GMM Davies-Bouldin Score: {davies_bouldin_gmm}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:22.203455Z","iopub.execute_input":"2024-11-12T17:48:22.203906Z","iopub.status.idle":"2024-11-12T17:48:22.330629Z","shell.execute_reply.started":"2024-11-12T17:48:22.203856Z","shell.execute_reply":"2024-11-12T17:48:22.328921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.scatterplot(x=tsne_features[:, 0], y=tsne_features[:, 1], hue=gmm_labels, palette='Set2')\nplt.title(\"TSNE GMM Cluster Visualization\")\nplt.xlabel(\"TSNE Component 1\")\nplt.ylabel(\"TSNE Component 2\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:48:22.337838Z","iopub.execute_input":"2024-11-12T17:48:22.339420Z","iopub.status.idle":"2024-11-12T17:48:22.915455Z","shell.execute_reply.started":"2024-11-12T17:48:22.339325Z","shell.execute_reply":"2024-11-12T17:48:22.914311Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are considering GaussianMixtureModel as our primary clustering model as it gives the highest **Silhouette score** and lowest **Davies-Bouldin** Score","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': unique_ids,\n    'Cluster': gmm_labels  \n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Submission file created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:54:34.836740Z","iopub.execute_input":"2024-11-12T17:54:34.837805Z","iopub.status.idle":"2024-11-12T17:54:34.851040Z","shell.execute_reply.started":"2024-11-12T17:54:34.837734Z","shell.execute_reply":"2024-11-12T17:54:34.849656Z"}},"outputs":[],"execution_count":null}]}